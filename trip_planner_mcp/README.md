# ğŸ§  Trip-Planner MCP Demo

A self-contained demo showing how Model Context Protocol (MCP) servers, agents, tools, and local LLMs work together to create a natural language travel assistant.

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Install and Start Ollama

#### Mac Installation (Homebrew)
```bash
# Install Ollama via Homebrew
brew install ollama

# Or download directly from https://ollama.ai
# curl -fsSL https://ollama.ai/install.sh | sh
```

#### Alternative Mac Installation (Direct Download)
```bash
# Download and install manually
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Start Ollama Service
```bash
# Start the Ollama service (runs in background)
ollama serve

# In a new terminal, download and run the llama3 model
ollama run llama3

# Verify installation
curl http://localhost:11434/api/tags
```

### 3. Start the Interactive CLI
```bash
# From the trip_planner_mcp directory
python client/mcp_client.py

# This will start an interactive session where you can type queries:
# Your query > weather in Kyoto
# Your query > 3-day Tokyo itinerary
# Your query > exit

# For one-off queries (old style):
python client/mcp_client.py query "weather in Kyoto" --verbose

# Get help
python client/mcp_client.py --help
```

## ğŸ“ Project Structure

```
trip_planner_mcp/
â”œâ”€â”€ mcp_server/          # MCP protocol handling
â”‚   â”œâ”€â”€ server.py        # Main server logic
â”‚   â””â”€â”€ schema.py        # Request/Response models
â”œâ”€â”€ agent/               # Agent with LLM reasoning
â”‚   â””â”€â”€ trip_planner_agent.py
â”œâ”€â”€ tools/               # Data fetching tools
â”‚   â”œâ”€â”€ weather_tool.py
â”‚   â”œâ”€â”€ hotspot_tool.py
â”‚   â””â”€â”€ restaurant_tool.py
â”œâ”€â”€ llm/                 # LLM integration
â”‚   â””â”€â”€ ollama_connector.py
â”œâ”€â”€ client/              # CLI interface
â”‚   â””â”€â”€ mcp_client.py
â”œâ”€â”€ data/                # Static JSON data
â”‚   â”œâ”€â”€ weather/
â”‚   â”œâ”€â”€ hotspots/
â”‚   â””â”€â”€ restaurants/
â””â”€â”€ tests/               # Test suite
    â””â”€â”€ test_end_to_end.py
```

## ğŸ¯ Supported Queries

- **Weather**: `"weather in Kyoto"`, `"what's the temperature in Tokyo?"`
- **Attractions**: `"top temples in Kyoto"`, `"must-see places in Tokyo"`
- **Restaurants**: `"best ramen in Kyoto"`, `"where to eat sushi in Tokyo"`
- **Trip Planning**: `"3-day Kyoto itinerary"`, `"plan a weekend trip to Tokyo"`

## ğŸ”§ How It Works

### Process Flow Diagram

```mermaid
graph TD
    A[ğŸ‘¤ User Query<br/>"weather in Kyoto"] --> B[ğŸ“± CLI Client<br/>mcp_client.py]
    B --> C[ğŸŒ MCP Server<br/>server.py]
    C --> D{ğŸ“‹ Request Validation<br/>- Length < 2000 chars<br/>- Valid task type<br/>- Non-empty query}
    
    D -->|âŒ Invalid| E[ğŸš« Error Response<br/>400 Bad Request]
    D -->|âœ… Valid| F[ğŸ§  Trip Planner Agent<br/>trip_planner_agent.py]
    
    F --> G[ğŸ¤– LLM Classification<br/>Ollama + llama3]
    G --> H{ğŸ” Intent Detection<br/>+ JSON Cleaning}
    
    H -->|ğŸŒ¤ï¸ weather_lookup| I[ğŸŒ¦ï¸ Weather Tool<br/>data/weather/]
    H -->|ğŸ›ï¸ hotspots_list| J[ğŸ—ºï¸ Hotspot Tool<br/>data/hotspots/]
    H -->|ğŸœ food_reco| K[ğŸ½ï¸ Restaurant Tool<br/>data/restaurants/]
    H -->|ğŸ“… trip_plan| L[ğŸ—“ï¸ Trip Planning<br/>Multi-tool coordination]
    H -->|â“ unknown| M[ğŸ¤· Unknown Handler<br/>Clarification request]
    
    I --> N[ğŸ¤– LLM Response Formatting<br/>Ollama + llama3]
    J --> N
    K --> N
    L --> N
    M --> N
    
    N --> O[ğŸ“¤ MCP Response<br/>Structured output]
    O --> P[ğŸ–¥ï¸ CLI Display<br/>Rich formatting]
    P --> Q[ğŸ‘¤ User sees result]
    
    style A fill:#e1f5fe
    style Q fill:#e8f5e8
    style G fill:#fff3e0
    style N fill:#fff3e0
    style H fill:#f3e5f5
```

### Step-by-Step Process

1. **User Query** â†’ CLI client wraps query in MCPRequest
2. **MCP Server** â†’ Validates request and passes to Agent
3. **Agent** â†’ Uses Ollama to classify intent (weather/food/etc)
4. **JSON Cleaning** â†’ Fixes malformed LLM responses with regex
5. **Tools** â†’ Fetch relevant data from JSON files
6. **Agent** â†’ Uses Ollama again to format natural response
7. **MCP Server** â†’ Returns structured MCPResponse
8. **CLI** â†’ Displays formatted result to user

## ğŸ§ª Running Tests

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest tests/ -v --cov=.
```

## ğŸ“ Adding New Cities

1. Create JSON files in `data/weather/`, `data/hotspots/`, and `data/restaurants/`
2. Follow the existing format (see Kyoto/Tokyo examples)
3. The tools will automatically detect new cities

## ğŸ› ï¸ Troubleshooting

### Common Issues
- **"Ollama request failed"**: Make sure Ollama is running (`ollama run llama3`)
- **"Cannot connect to Ollama"**: Verify Ollama is running on localhost:11434
- **"Weather data not available"**: Check that city name matches JSON filename (kyoto.json, tokyo.json)
- **"City name is required"**: Make sure to specify a city in your query
- **Import errors**: Run from the `trip_planner_mcp` directory

### Error Handling Features
The demo includes comprehensive error handling:
- **Input validation**: Prevents malformed requests and excessive query lengths
- **Path sanitization**: Protects against directory traversal attacks
- **LLM timeouts**: 30-second timeout prevents hanging requests
- **JSON cleaning**: Regex-based cleaning fixes malformed LLM responses (NEW!)
- **Graceful degradation**: Provides fallback responses when LLM formatting fails
- **Data validation**: Ensures JSON files are properly formatted
- **Connection resilience**: Clear error messages for Ollama connectivity issues

### Recent Improvements
- **Fixed "Classification failed" errors**: Added robust JSON cleaning to handle malformed LLM responses
- **Enhanced reliability**: System now handles explanatory text in JSON values like `"city": "KYOTO" (capitalized)`
- **Better debugging**: Improved logging shows raw LLM responses and cleaned JSON

## ğŸ“ Learning Points

This demo teaches:
- How MCP servers wrap and structure requests
- How agents coordinate LLM reasoning with tool calls
- How to use local LLMs (Ollama) for both classification and generation
- How to build modular, testable architectures
- End-to-end flow with no external APIs required